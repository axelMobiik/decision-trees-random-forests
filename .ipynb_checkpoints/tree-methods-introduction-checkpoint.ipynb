{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7443e1e-5d58-4f48-a9d0-8f1b86094d22",
   "metadata": {},
   "source": [
    "<h1>Introduction to Tree Methods</h1>\n",
    "<p>\n",
    "    Let's start off with a thought experiment to give some motivation behind using a decision tree method.\n",
    "    <br>Imagine that i play tennis every saturday and i always invite a friend to come with me.\n",
    "    <br>Simetimes my friend shows up, sometimes not.\n",
    "    <br>For him it depends on a variety of factors, such as: wather, temperature, humidiy, wind, etc.\n",
    "    <br>I start keeping track of these features and whether or not he showed up to play with me.\n",
    "</p>\n",
    "<p>\n",
    "    I want to use this data to predict whether or not he will show up to play.\n",
    "    <br>An intuitive way to do this is through a Decision Tree\n",
    "    <br>In this tree we have:\n",
    "    <li>Root</li>\n",
    "    <li>> The node that performs the first split</li>\n",
    "    <li>Leaves</li>\n",
    "    <li>> Terminal nodes that predict the outcome</li>\n",
    "</p>\n",
    "<p>\n",
    "    Imaginary Data with 3 features (X, Y and Z) with two possible clases (A, B)\n",
    "    <br>Splitting on Y gives us a clear separation between classes\n",
    "    <br>We could have also tried splitting on other features\n",
    "</p>\n",
    "<p>\n",
    "    Entropy and Information Gain are the Mathematical Methods of choosing the best split. Refer to reading assignment\n",
    "</p>\n",
    "<p>\n",
    "    To improve performance, we can use many trees with a random sample of features chosen as the split\n",
    "    <li>A new random sample of features is chosen for <strong>every single tree at every single split</strong></li>\n",
    "    <li>For <strong>classification</strong>, m is typically chosen to be the square root of p</li>\n",
    "</p>\n",
    "<p>\n",
    "    What's the point?\n",
    "    <br>Suppose there is <strong>one very strong feature</strong> in the data set. When using \"bagged\" trees, most of the trees will use that feature as the top split, resulting in an ensemble of similar trees that are <strong>highly correlated</strong>\n",
    "    <br>Averaging highly correlated quantities does not significantly reduce variance.\n",
    "    <br>By rnadomly leaving out candidate features from each split, <strong>Random Forests \"decorrelates\" the trees</strong>, such that the averaging process can reduce the variance of the resulting model.\n",
    "</p>\n",
    "<p>\n",
    "    We'll start by taking a look at a small example data set of Kyphosis (a medical spine condition) patients and try to predict whether or not a correctives spine surgery was successful.\n",
    "    <br>Then for your portfolio project we'll use loan data from Lending Club to predict default rates.\n",
    "</p>\n",
    "</p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
